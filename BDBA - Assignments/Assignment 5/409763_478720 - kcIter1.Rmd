---
title: "Weekly Assignment 5"
author: "Loïc Roldán Waals  & Kimberly Cheng"
date: "Wednesday 25 April, 2018"
output:
  html_document:
    pdf_print: paged
  toc: TRUE
  pdf_document: default
---

##Q1

**Using a database with at least 200 selected tweets from previous weekly assignments, clean up your data set and build a term frequency matrix.**

```{r message = FALSE, warning = FALSE}
#--------------------------------------
# Load Relevant Packages
#--------------------------------------

library(stringr)
library(tm)
library(data.table)


# Import Selected Tweets form Previous Weekly Assignments
AITweets <- read_delim("AITweets.csv", ";",
                       escape_double = FALSE,
                       trim_ws = TRUE)

AITweets$text <- as.character(AITweets$text)

# Make Data Available for 
AITweets <- as.list(AITweets)
vecData  <- VectorSource(AITweets$text)
myCorpus <- VCorpus(vecData)

#--------------------------------------
# Clean Up Your Data Set
#--------------------------------------

# Make a function to remove urls
removeLinks <- function(x) {
  gsub("http[^[:blank:]]+","",x)
}

# Removing hyperlinks from the corpus
myCorpus <- tm_map(myCorpus, content_transformer(removeLinks))

# Make a function to ensure strings are converted to UTF
convUTF <- function(x) { 
  iconv(x, "ASCII", "UTF-8")
}

# Ensure Strings are Converted to UTF
myCorpus <- tm_map(myCorpus, content_transformer(convUTF))

# Convert words to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

# Remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# Remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)

# Remove stop-words
# Define vector of words to remove
stopWords <- c("i","m","rotterdam","frh","at","in","a")
myCorpus <- tm_map(myCorpus, removeWords, stopwords(stopWords))

# Remove extra white spaces
myCorpus <- tm_map(myCorpus, stripWhitespace)

# Stemming
myCorpus<- tm_map(myCorpus, stemDocument, language = "english")

#--------------------------------------
# Build a Term Frequency Matrix
#--------------------------------------

# Matrix with Term Frequency
dtm <- TermDocumentMatrix(myCorpus)
dtm

```

**(a) Cluster the resulting documents using the k-means algorithm with a suitable distance metric. Explain in plain text the information potentially encoded in each cluster. Tip: you can use decision trees to help you with this task.**

```{r}
#--------------------------------------
# Assess the Optimal Number of Clusters with the Elbow Method
#--------------------------------------

rsltKmeans <- kmeans(dtm, 5)

# Cluster Plot against 1st 2 principal components
clusplot(dtm, rsltKmeans$cluster, 
         color = TRUE, shade = TRUE, 
         labels = 2, lines = 0)


```

**(b) Manually label all the tweets in your data set as positive (label = 1), negative (label = -1), and neutral (label = 0). Thereafter, using 10-fold cross validation, report the overall accuracy of a Naive Bayes model when predicting the target (labels). Tip: you might (or might not) have to reduce the sparsity of the term frequency matrix to get higher accuracy.**

```{r}

# Remove Sparse Terms
dtm2 <- removeSparseTerms(dtm, sparse = 0.9)

# Redefine dtm2 as a matrix
dtm2 <- as.matrix(dtm2) 

#--------------------------------------
# Naive Bayes Classifier
#--------------------------------------

#Randomize the order of observations
dtm <- dtm[sample(1:nrow(dtm)), ]

# Create 10 equally sized folds
n.Folds <- 10
Folds.tweets <- cut(seq(1, nrow(dtm)), 
                     breaks = n.Folds, 
                     labels = FALSE)

# Create Empty Vectors to Collect Results
accuracy.NB <- rep(NA, n.Folds)
specificity.NB <- rep(NA, n.Folds)
sensitivity.NB <- rep(NA, n.Folds)

# Perform the 10-fold cross validation for Naive Bayes

for(i in 1 : n.Folds) { 
  testIndexes <- which(Folds.tweets == i, arr.ind = TRUE)
  df.tweet.ai.test <- dtm[testIndexes, ]
  df.tweet.ai.train <- dtm[-testIndexes, ]
  
  # Fit the NB Model
  rslt.NB <- naiveBayes(mdl.tweet.ai, data = df.tweet.ai.train)
  
  # Predict Classification for Test Set
  pred.NB <- predict(rslt.NB, df.tweet.ai.test, type = "class")
  
  # Overall Accuracy
  accuracy.NB[i] <- mean(pred.NB == df.tweet.ai.test)
  
  specificity.NB[i] <- specificity(df.tweet.ai.test, 
                                   pred.NB, 
                                   positive = "1")
  
  sensitivity.NB[i] <- specificity(df.tweet.ai.test, 
                                   pred.NB, 
                                   negative = "-1")
  
  }

```

##Q2

**Collect the news from News API about Volkswagen, Toyota, Honda, BMW, Mercedes-Benz, Ford, Volvo, and Tesla over the past 150 days.**

```{r}
#--------------------------------------
# Load Relevant Packages
#--------------------------------------
library(jsonlite)

# Load Current Date
now <- Sys.Date()

# Chosen Topic
topic <- "Volkswagen"
urlBefore <- "https://newsapi.org/v2/everything"

# Empty Data frame to collect data
jsonNews <- data.frame()
#define an empty jsonNews dataframe
vbls <- c("author", "title",  "description", "publishedAt", "url")

for (d in 0:150) {
   # Define the from and to dates, and print to console in order
   # to keep track of the process
   fromDate <- strftime(now - d,  format = "%Y-%m-%d")
   toDate   <- strftime(fromDate, format = "%Y-%m-%d")
   cat("From", fromDate, "to", toDate, "\n")
   
   # Define the url, which is used to get the date from the api
   url <- paste0(urlBefore, 
                 "?q=", topic,
                 "&from=", fromDate,
                 "&to=", toDate,
                 "&sortBy=publishedAt", 
                 "&apiKey=654da5783dbb42f7870ca130101a196d") 
   
   jsonTmp  <- fromJSON(url)
   
   #This part defines the URL ^  ^ if you want to just check how it works copy it and put it in your browser && then you get     the results of your call // messages
   #-----------------------------------------
   # Convert the resulting json object to a data frame and add 
   # it to the file with the results
   
   #Result taking from a data frame and stored in jsonNews 
   
   dataTmp  <- as.data.frame(jsonTmp$articles[vbls])
   jsonNews <- rbind(jsonNews,dataTmp)
}

# Save the File for Later Use
save(jsonNews, file = "NewsFeed_Assignment.rda")

```

**(a) Use the difference between the number of positive and negative words as the definition of sentiment. Which company has more positive news? Plot a boxplot with the resulting scores.**

```{r}

```


**(b)Based on the scores obtained in previous step, decide by yourself the appropriate cutoffs to label all news as positive (label = 1), neutral (label = 0), and negative (label = -1). Thereafter, using 10-fold cross validation, compare the accuracy of a Naive Bayes model against a SVM model when predicting the target (labels). Tip: you might (or might not) have to implicitly select variables by reducing the sparsity of the term frequency matrix in order to speed up the learning process.**

```{r}

```



