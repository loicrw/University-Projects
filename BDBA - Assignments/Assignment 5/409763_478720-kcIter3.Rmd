---
output:
  pdf_document: default
  html_document: default
---

##Q2

**Collect the news from News API about Volkswagen, Toyota, Honda, BMW, Mercedes-Benz, Ford, Volvo, and Tesla over the past 150 days.**

```{r results = "hide", warning = FALSE, message = FALSE, include = FALSE, eval = FALSE}
#--------------------------------------
# Load Relevant Packages
#--------------------------------------
library(jsonlite)

# Load Current Date
now <- Sys.Date()

# Chosen Topic
urlBefore <- "https://newsapi.org/v2/everything"

#--------------------------------------
# Create Function to collect the News
#--------------------------------------
news_api_call <- function(car) { 
  for (d in 0:150) {
    # Create an Empty Data frame to collect data
    jsonNews <- data.frame()
    vbls <- c("author", "title",  "description", "publishedAt", "url")
    
    # Define the From and To dates, and print to console in order
    # to keep track of the process
    fromDate <- strftime(now - d,  format = "%Y-%m-%d")
    toDate   <- strftime(fromDate, format = "%Y-%m-%d")
    cat("From", fromDate, "to", toDate, "\n")
    
    # Define the URL, which is used to get the date from the api
    url <- paste0(urlBefore, 
                  "?q=", car,
                  "&from=", fromDate,
                  "&to=", toDate,
                  "&sortBy=publishedAt", 
                  "&apiKey=f02b8d39b61c4290b4014810d3ef1060") 
    
    jsonTmp  <- fromJSON(url)
    
  } 
  
  # Take results from data frame and combine it into jsonNews
  dataTmp <- as.data.frame(jsonTmp$articles[vbls])
  jsonNews <- rbind(jsonNews, dataTmp)
  
}

#--------------------------------------
# Call Function for Each of the Keywords
#--------------------------------------

df.Volkswagen <- news_api_call("volkswagen")
df.Toyota <- news_api_call("toyota")
df.Honda <- news_api_call("honda")
df.BMW <- news_api_call("bmw")
df.MercedesBenz <- news_api_call("mercedes-benz")
df.Ford <- news_api_call("ford")
df.Volvo <- news_api_call("volvo")
df.Tesla <- news_api_call("tesla")

# Save collected news data for later use
save(list = c("df.Volkswagen", "df.Toyota", "df.Honda", "df.BMW", "df.MercedesBenz", "df.Ford", "df.Volvo", "df.Tesla"), file = "cars.rda")

```

**(a) Use the difference between the number of positive and negative words as the definition of sentiment. Which company has more positive news? Plot a boxplot with the resulting scores.**

```{r message = FALSE, warning = FALSE}
#--------------------------------------
# Load Relevant Files and Packages
#--------------------------------------
load(file = "cars.rda")

# Read lines of Positive and Negative Words
positive_words <- readLines("positive_words.txt")
negative_words <- readLines("negative_words.txt") 

library(stringi)
library(tm)

#--------------------------------------
# Prepare Corpus and Clean Textual Data
#--------------------------------------

# Remove links Function 
removeLinks <- function (x) {
  gsub("http [^[:blank:]]+","",x)
}

# Create a function to convert all strings to UTF 
convertUTF <- function(d) { 
  d$description <- iconv(d$description, "latin1", "ASCII")
  d$description <- iconv(d$description, "ASCII", "UTF-8")
  # Extract only complete cases
  d <- d[complete.cases(d), ]
}

# Create a function to Clean each Corpus 
clean_corpus <- function(x) { 
  
  x <- convertUTF(x)
  
  vecData <- VectorSource(x$description)
  myCorpus <- VCorpus(vecData)
  
  # Remove Links
  myCorpus <- tm_map(myCorpus, content_transformer(removeLinks))
  
  # Convert words to lowercase
  myCorpus <- tm_map(myCorpus, content_transformer(stri_trans_tolower))
  
  # Remove punctuation
  myCorpus <- tm_map(myCorpus, removePunctuation)
  
  # Remove numbers
  myCorpus <- tm_map(myCorpus, removeNumbers)
  
  # Remove stop-words
  myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
  
  # Remove extra white spaces
  myCorpus <- tm_map(myCorpus, stripWhitespace)
  
  # Stemming
  myCorpus<- tm_map(myCorpus, stemDocument, language = "english")
  
}

# Run function to clean corpus
myCorpus.Volkswagen <- clean_corpus(df.Volkswagen)
myCorpus.Toyota <- clean_corpus(df.Toyota)
myCorpus.Honda <- clean_corpus(df.Honda)
myCorpus.BMW <- clean_corpus(df.BMW)
myCorpus.Ford <- clean_corpus(df.Ford)
myCorpus.MercedesBenz <- clean_corpus(df.MercedesBenz)
myCorpus.Volvo <- clean_corpus(df.Volvo)
myCorpus.Tesla <- clean_corpus(df.Tesla)
```

```{r}
#--------------------------------------
# Score Each Corpus
#--------------------------------------

# Create a function to Score each Corpus

score_corpus <- function(y) { 
  # Initiate the Score Topic
  scoreTopic <- 0
  
  #Start a loop over the documents
  for(ifold in 1: length(y)) {
    terms <- unlist(strsplit(y[[ifold]]$content, " "))
    pos_matches <- sum(terms %in% positive_words)
    neg_matches <- sum(terms %in% negative_words)
    scoreTopic[ifold] <- pos_matches - neg_matches
  } # End of the for loop
  
  df <- data.frame(text = sapply(y, paste, collapse = " "), stringsAsFactors = FALSE)
  df$score_topic <- scoreTopic
} 

score.Volkswagen <- score_corpus(myCorpus.Volkswagen)
score.Toyota <- score_corpus(myCorpus.Toyota)
score.Honda <- score_corpus(myCorpus.Honda)
score.BMW <- score_corpus(myCorpus.BMW)
score.Ford <- score_corpus(myCorpus.Ford)
score.MercedesBenz <- score_corpus(myCorpus.MercedesBenz)
score.Volvo <- score_corpus(myCorpus.Volvo)
score.Tesla <- score_corpus(myCorpus.Tesla)

```


```{r message = FALSE, warning = FALSE, results = 'asis'}

df <- cbind(score.Tesla, score.BMW, score.Ford, score.Honda, 
            score.Toyota, score.MercedesBenz, score.Volkswagen, score.Volvo)

#--------------------------------------
# Evaluate Sentiment
#--------------------------------------

# Total Sum of Scores of Topics (Positive Words - Negative Words)
sums <- data.frame(Sums = colSums(df, na.rm = TRUE))
sums

means <- data.frame(Means = colMeans(df, na.rm = TRUE))
means

```

When evaluating which brand has the most positive sentiment, we initially evaluated the sum of the sentiment where (sentiment = positive words - negative words). The highest in this data is Mercedes Benz with a sentiment score from the whole corpus of 108. This means that Mercedes Benz has the most positive terms relative to negative terms out of all brands. When evaluating the average sentiment score per document, it is seen that again Mercedes Benz has the largest score with an average 0.18 score out of all documents within the corpus. In order to get better visualization of the distribution of these sentiment scores, a boxplot was plotted. 


```{r}
#--------------------------------------
# Plot Boxplot
#--------------------------------------
par(mar = c(10,4,4,2) + 0.1)
boxplot(df, ylab =  "Topic Score (Positive Word - Negative Word)", las = 2)
```
From the boxplot, it is visible that the median sentiment score for the documents within each corpus all fall around the same range ~0. The distribution of sentiment scores appear to be largely skewed with only Tesla showing a relatively normal distribution. Ford, Honda, Toyota, Mercedes-Benz, and Volkswagen are all more right-skewed, while Volvo is left-skewed. 

**(b)Based on the scores obtained in previous step, decide by yourself the appropriate cutoffs to label all news as positive (label = 1), neutral (label = 0), and negative (label = -1). Thereafter, using 10-fold cross validation, compare the accuracy of a Naive Bayes model against a SVM model when predicting the target (labels). Tip: you might (or might not) have to implicitly select variables by reducing the sparsity of the term frequency matrix in order to speed up the learning process.**

```{r warning = FALSE, message = FALSE}
#--------------------------------------
# Prepare Term Frequency Matix into Data Frame
#--------------------------------------

# Create a Function to convert the corpus to a data frame 
# with the calculated sentiment scores

convert2DF <- function(a, b) { 
  df <- cbind(data.frame(text = sapply(a, paste, collapse = " "), 
                         stringsAsFactors = FALSE), score_topic = b)
}

# Run the functions for each brand
df.Volkswagen.Corp <- convert2DF(myCorpus.Volkswagen, score.Volkswagen)
df.Toyota.Corp <- convert2DF(myCorpus.Toyota, score.Toyota)
df.Honda.Corp <- convert2DF(myCorpus.Honda, score.Honda)
df.BMW.Corp <- convert2DF(myCorpus.BMW, score.BMW)
df.MercedesBenz.Corp <- convert2DF(myCorpus.MercedesBenz, score.MercedesBenz)
df.Ford.Corp <- convert2DF(myCorpus.Ford, score.Ford)
df.Volvo.Corp <- convert2DF(myCorpus.Volvo, score.Volvo)
df.Tesla.Corp <- convert2DF(myCorpus.Tesla, score.Tesla)


# Merge all dataframes
df <- rbind(df.Volkswagen.Corp, df.Toyota.Corp, 
            df.Honda.Corp, df.Ford.Corp, df.BMW.Corp, 
            df.MercedesBenz.Corp, df.Tesla.Corp, df.Volvo.Corp)

# Classify Sentiment based on cutoffs
# Cutoffs ( > 1 is "1", < 1 is "-1", and 0 is "0")
df$sentiment[df$score_topic > 0] <- 1
df$sentiment[df$score_topic < 0] <- -1
df$sentiment[df$score_topic == 0] <- 0

# Convert Back to Corpus
vecData <- VectorSource(df$text)
myCorpus <- VCorpus(vecData)

# Convert to Document Term Matrix - 
dtm <- DocumentTermMatrix(myCorpus)

# Remove Sparse Terms
dtm2 <- removeSparseTerms(dtm, sparse = 0.99)
dtm2 <- as.matrix(dtm2)

# Create Data Frame for Predictions
df.news <- data.frame(cbind(dtm2, sentiment = df$sentiment))
df.news$sentiment <- as.factor(df.news$sentiment)

#----------------------------------
# 10-Fold Cross Validation - Prep
#----------------------------------

# Prepare Cross Validation
# 1 - Randomize the order of the observations
df.news <- df.news[sample(1:nrow(df.news)), ]

# 2 - Create 10 equally sized folds - (10 = K)
n.Folds <- 10
Folds<- cut(seq(1, nrow(df.news)), 
            breaks = n.Folds, 
            labels = FALSE)

```

```{r warning = FALSE, message = FALSE}
#----------------------------------
# Naive Bayes Classifier 
#----------------------------------

library(caret)
library(e1071)

# Define the Model

mdl.news <- sentiment ~ . 

# Create Empty Vectors to Collect Results
accuracy.NB <- rep(NA, n.Folds)
accuracy.SVM <- rep(NA, n.Folds)

# Perform the 10-fold cross validation for Naive Bayes
#Set a threshold 
nbt = 0.5

for(i in 1: n.Folds) { 
  
  testIndexes <- which(Folds == i, arr.ind = TRUE)
  df.news.test <- df.news[testIndexes, ]
  df.news.train <- df.news[-testIndexes, ]
  
  #Fit NB Model
  rslt.NB <- naiveBayes(mdl.news, data = df.news.train)
  #Fit SVM Model
  rslt.SVM <- svm(mdl.news, data = df.news.train, type = "C-classification", probability = TRUE)
  
  #Predicted Classification Test Set
  pred.NB <- predict(rslt.NB, df.news.test, type = "class", threshold = nbt)
  pred.SVM <- predict(rslt.SVM, df.news.test, type = "response")
  
  #Accuracy
  accuracy.NB[i] <- mean(pred.NB == df.news.test$sentiment)
  accuracy.SVM[i] <- mean(pred.SVM == df.news.test$sentiment)
  
}

# Calculate Results for Naive Bayes Model: Accuracy
JSONAccNB <- mean(accuracy.NB)
JSONAccNB

JSONAccSVM <- mean(accuracy.SVM)
JSONAccSVM
```

In this data, SVM has the highest accuracy in predicting the sentiment for the news articles based on textual data with a mean accuracy of `mean(accuracy.SVM)` over a Naive Bayes which has a mean accuracy of `mean(accuracy.SVM)`. 
